{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import time\n",
    "from multiprocessing.dummy import Pool\n",
    "K.set_image_data_format('channels_first')\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import tensorflow as tf\n",
    "from fr_utils import *\n",
    "from inception_blocks_v2 import *\n",
    "import win32com.client as wincl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PADDING = 50\n",
    "ready_to_detect_identity = True\n",
    "windows10_voice_interface = wincl.Dispatch(\"SAPI.SpVoice\")\n",
    "\n",
    "FRmodel = faceRecoModel(input_shape=(3, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.3):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss as defined by formula (3)\n",
    "    \n",
    "    Arguments:\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
    "            positive -- the encodings for the positive images, of shape (None, 128)\n",
    "            negative -- the encodings for the negative images, of shape (None, 128)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    # Step 1: Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=-1)\n",
    "    # Step 2: Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=-1)\n",
    "    # Step 3: subtract the two previous distances and add alpha.\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
    "    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0.0))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
    "load_weights_from_FaceNet(FRmodel)\n",
    "\n",
    "def prepare_database():\n",
    "    database = {}\n",
    "\n",
    "    # load all the images of individuals to recognize into the database\n",
    "    for file in glob.glob(\"images/*\"):\n",
    "        identity = os.path.splitext(os.path.basename(file))[0]\n",
    "        database[identity] = img_path_to_encoding(file, FRmodel)\n",
    "\n",
    "    return database\n",
    "\n",
    "def webcam_face_recognizer(database):\n",
    "    \"\"\"\n",
    "    Runs a loop that extracts images from the computer's webcam and determines whether or not\n",
    "    it contains the face of a person in our database.\n",
    "\n",
    "    If it contains a face, an audio message will be played welcoming the user.\n",
    "    If not, the program will process the next frame from the webcam\n",
    "    \"\"\"\n",
    "    global ready_to_detect_identity\n",
    "\n",
    "    cv2.namedWindow(\"preview\")\n",
    "    vc = cv2.VideoCapture(0)\n",
    "\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    while vc.isOpened():\n",
    "        _, frame = vc.read()\n",
    "        img = frame\n",
    "\n",
    "        # We do not want to detect a new identity while the program is in the process of identifying another person\n",
    "        if ready_to_detect_identity:\n",
    "            img = process_frame(img, frame, face_cascade)   \n",
    "        \n",
    "        key = cv2.waitKey(100)\n",
    "        cv2.imshow(\"preview\", img)\n",
    "\n",
    "        if key == 27: # exit on ESC\n",
    "            break\n",
    "    cv2.destroyWindow(\"preview\")\n",
    "\n",
    "def process_frame(img, frame, face_cascade):\n",
    "    \"\"\"\n",
    "    Determine whether the current frame contains the faces of people from our database\n",
    "    \"\"\"\n",
    "    global ready_to_detect_identity\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # Loop through all the faces detected and determine whether or not they are in the database\n",
    "    identities = []\n",
    "    for (x, y, w, h) in faces:\n",
    "        x1 = x-PADDING\n",
    "        y1 = y-PADDING\n",
    "        x2 = x+w+PADDING\n",
    "        y2 = y+h+PADDING\n",
    "\n",
    "        img = cv2.rectangle(frame,(x1, y1),(x2, y2),(255,0,0),2)\n",
    "\n",
    "        identity = find_identity(frame, x1, y1, x2, y2)\n",
    "\n",
    "        if identity is not None:\n",
    "            identities.append(identity)\n",
    "\n",
    "    if identities != []:\n",
    "        cv2.imwrite('example.png',img)\n",
    "\n",
    "        ready_to_detect_identity = False\n",
    "        pool = Pool(processes=1) \n",
    "        # We run this as a separate process so that the camera feedback does not freeze\n",
    "        pool.apply_async(welcome_users, [identities])\n",
    "    return img\n",
    "\n",
    "def find_identity(frame, x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "    Determine whether the face contained within the bounding box exists in our database\n",
    "\n",
    "    x1,y1_____________\n",
    "    |                 |\n",
    "    |                 |\n",
    "    |_________________x2,y2\n",
    "\n",
    "    \"\"\"\n",
    "    height, width, channels = frame.shape\n",
    "    # The padding is necessary since the OpenCV face detector creates the bounding box around the face and not the head\n",
    "    part_image = frame[max(0, y1):min(height, y2), max(0, x1):min(width, x2)]\n",
    "    \n",
    "    return who_is_it(part_image, database, FRmodel)\n",
    "\n",
    "def who_is_it(image, database, model):\n",
    "    \"\"\"\n",
    "    Implements face recognition for the happy house by finding who is the person on the image_path image.\n",
    "    \n",
    "    Arguments:\n",
    "    image_path -- path to an image\n",
    "    database -- database containing image encodings along with the name of the person on the image\n",
    "    model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "    min_dist -- the minimum distance between image_path encoding and the encodings from the database\n",
    "    identity -- string, the name prediction for the person on image_path\n",
    "    \"\"\"\n",
    "    encoding = img_to_encoding(image, model)\n",
    "    \n",
    "    min_dist = 100\n",
    "    identity = None\n",
    "    \n",
    "    # Loop over the database dictionary's names and encodings.\n",
    "    for (name, db_enc) in database.items():\n",
    "        \n",
    "        # Compute L2 distance between the target \"encoding\" and the current \"emb\" from the database.\n",
    "        dist = np.linalg.norm(db_enc - encoding)\n",
    "\n",
    "        print('distance for %s is %s' %(name, dist))\n",
    "\n",
    "        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "    \n",
    "    if min_dist > 0.52:\n",
    "        return None\n",
    "    else:\n",
    "        return str(identity)\n",
    "\n",
    "def welcome_users(identities):\n",
    "    \"\"\" Outputs a welcome audio message to the users \"\"\"\n",
    "    global ready_to_detect_identity\n",
    "    welcome_message = 'Welcome '\n",
    "\n",
    "    if len(identities) == 1:\n",
    "        welcome_message += '%s, have a nice day.' % identities[0]\n",
    "    else:\n",
    "        for identity_id in range(len(identities)-1):\n",
    "            welcome_message += '%s, ' % identities[identity_id]\n",
    "        welcome_message += 'and %s, ' % identities[-1]\n",
    "        welcome_message += 'have a nice day!'\n",
    "\n",
    "    windows10_voice_interface.Speak(welcome_message)\n",
    "\n",
    "    # Allow the program to start detecting identities again\n",
    "    ready_to_detect_identity = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "database = prepare_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shravan': array([[-0.0134197 ,  0.13121757,  0.04921615,  0.09393005,  0.05515148,\n",
       "          0.16014406, -0.00431549, -0.13946865,  0.03062751, -0.10061101,\n",
       "          0.01158424,  0.01605615, -0.02707168,  0.02049754,  0.12924995,\n",
       "         -0.06459902, -0.02945597, -0.04673238, -0.04341355,  0.04203771,\n",
       "         -0.04280964, -0.04420829,  0.06789171,  0.1006868 ,  0.12275226,\n",
       "         -0.13592601, -0.09958978, -0.02715107, -0.01183988, -0.00104117,\n",
       "          0.16916962,  0.09368462, -0.00386403,  0.09481067,  0.01046085,\n",
       "          0.12675951,  0.05898423,  0.02018897, -0.00494771, -0.02543603,\n",
       "          0.12620544, -0.05997596,  0.01137988, -0.15150112, -0.09615368,\n",
       "          0.00702265,  0.07262491,  0.01016898, -0.0901181 ,  0.03437148,\n",
       "         -0.09588671, -0.0107567 ,  0.10159862, -0.0339659 ,  0.03160702,\n",
       "          0.07656354, -0.11190262,  0.13013189, -0.09480862, -0.02312038,\n",
       "         -0.18703748,  0.12428505,  0.0828784 , -0.2863752 ,  0.0999281 ,\n",
       "          0.25308672,  0.05975378, -0.02383109, -0.25261396,  0.09264482,\n",
       "         -0.00732998,  0.07188542, -0.07943987,  0.08932425,  0.15473968,\n",
       "          0.02083702,  0.03614043, -0.11716881,  0.01037777,  0.02296124,\n",
       "         -0.08008166,  0.02942105, -0.03320418, -0.06780307, -0.02606422,\n",
       "          0.03177825, -0.05399634, -0.03275953, -0.08741656,  0.10012489,\n",
       "          0.14408064, -0.1326572 ,  0.095764  ,  0.0393778 ,  0.01853593,\n",
       "         -0.02444939, -0.05272599,  0.00619563,  0.05969373,  0.00238744,\n",
       "         -0.05876932, -0.08245593, -0.0808199 ,  0.03829045, -0.07176418,\n",
       "          0.18796405,  0.02074771, -0.01010145, -0.01368477, -0.01800797,\n",
       "         -0.00904465,  0.10356852,  0.03464783,  0.00321226,  0.04705632,\n",
       "         -0.05167321, -0.08732709,  0.04281526,  0.12343912,  0.18335617,\n",
       "          0.11015539, -0.03399042, -0.03919194,  0.08127764,  0.03386686,\n",
       "          0.10656004,  0.07140028, -0.05994828]], dtype=float32),\n",
       " 'skuli': array([[-0.00498306,  0.11184471, -0.07694124,  0.00299568,  0.01354921,\n",
       "          0.15663935, -0.07720196, -0.10385437, -0.11522179,  0.02640037,\n",
       "          0.03666251,  0.00238365,  0.1388475 ,  0.01889619,  0.1000767 ,\n",
       "          0.00609542,  0.0335596 , -0.03925608,  0.01391244,  0.14941332,\n",
       "         -0.01975742, -0.05641627, -0.02548973,  0.13169992,  0.00034713,\n",
       "          0.02477526, -0.1140675 , -0.11487041,  0.0079274 ,  0.08311953,\n",
       "          0.04537308,  0.15815604,  0.00864166,  0.05631171, -0.0285395 ,\n",
       "          0.17198965,  0.07316542, -0.01775573,  0.00803989, -0.03655607,\n",
       "          0.04948125, -0.11888205,  0.04972624, -0.06761312, -0.0332898 ,\n",
       "          0.0700791 ,  0.21507779,  0.06836393, -0.18466294, -0.03396951,\n",
       "         -0.0535431 , -0.1137716 ,  0.0968542 ,  0.00242642,  0.07934634,\n",
       "          0.08762056, -0.13034678,  0.06273901,  0.0378316 , -0.11623774,\n",
       "         -0.12292229,  0.13694574,  0.08005308, -0.19516271,  0.01714239,\n",
       "          0.20821413,  0.07996168,  0.01139671, -0.1101128 , -0.14269675,\n",
       "         -0.0513728 , -0.00681447, -0.07422774, -0.0351094 ,  0.18060513,\n",
       "          0.05728734, -0.03191426, -0.00299977,  0.07263883,  0.07002489,\n",
       "          0.03621626,  0.04359882, -0.10599206, -0.08236191, -0.0444365 ,\n",
       "          0.15696016, -0.02384515, -0.07519027, -0.02233067,  0.11172421,\n",
       "          0.08013577, -0.08214533,  0.050234  ,  0.09216829, -0.11651156,\n",
       "          0.02129854, -0.04097575,  0.05555611,  0.05297714, -0.07159326,\n",
       "          0.01186529, -0.03439081, -0.02788838,  0.01987347, -0.17033209,\n",
       "          0.2112346 , -0.09282547,  0.04011076, -0.1030753 ,  0.02928646,\n",
       "          0.02498703,  0.0644672 , -0.03154958, -0.11397092, -0.06407884,\n",
       "         -0.01998508, -0.18965288, -0.08034117, -0.02046708,  0.06194498,\n",
       "          0.0647513 , -0.10084391,  0.05738395,  0.08010999, -0.07895138,\n",
       "          0.1237378 , -0.04695773, -0.06885175]], dtype=float32)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "webcam_face_recognizer(database)\n",
    "\n",
    "# ### References:\n",
    "# \n",
    "# - Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n",
    "# - Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf (2014). [DeepFace: Closing the gap to human-level performance in face verification](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf) \n",
    "# - The pretrained model we use is inspired by Victor Sy Wang's implementation and was loaded using his code: https://github.com/iwantooxxoox/Keras-OpenFace.\n",
    "# - Our implementation also took a lot of inspiration from the official FaceNet github repository: https://github.com/davidsandberg/facenet \n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
